-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
Aim: Introduction to Business Analytics: Exploratory Data Analysis (EDA) on Employee Data
Objective: To collect data from the "employees.csv" dataset and perform exploratory data analysis to gain initial insights, including handling missing values.
-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
# Step 1: Importing Libraries, Datasets, and Printing First 5 Entries
import pandas as pd
import numpy as np
df = pd.read_csv("employees.csv")
print("First 5 entries of the DataFrame:")
print(df.head())

# Step 2: Showing Total Rows and Columns in the Dataset
print("\nShape of the DataFrame (Rows, Columns):")
print(df.shape)

# Step 3: Showing Summary Statistics of Numerical Columns
print("\nSummary statistics of numerical columns:")
print(df.describe())

# Step 4: Showing Information about the DataFrame
print("\nInformation about the DataFrame (column names, non-null counts, data types, memory usage):")
df.info()

# Step 5: Showing the Number of Unique Entries in Each Column
print("\nNumber of unique entries in each column:")
print(df.nunique())

# Step 6: Showing the Total Number of Null Values in Each Column
print("\nTotal number of null values in each column:")
print(df.isnull().sum())

# Step 7: Replacing Null Values in the 'Gender' Column
df["Gender"].fillna("No Gender", inplace=True)
print("\nTotal number of null values after filling 'Gender' NaNs:")
print(df.isnull().sum())

# Step 8: Removing Rows with Any Null Values and Displaying Updated Null Counts and Shape
df_dropped = df.dropna(axis=0, how="any")
print("\nTotal number of null values after dropping rows with NaNs:")
print(df_dropped.isnull().sum())
print("\nShape of the DataFrame after dropping rows with NaNs (Rows, Columns):")
print(df_dropped.shape)
-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
Aim: Analyze data to identify trends and patterns that can be used for business decision-making.
Dataset Description: In this practical, we use two datasets: "employees.csv" and "Iris.csv", which contain valuable data.
-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
Part 1: Analyzing "Iris.csv"
# Step 1: Importing pandas, numpy libraries and importing dataset “Iris.csv”
# Using filter() to get some specific columns from the dataset
import numpy as np
import pandas as pd

df_iris = pd.read_csv("Iris.csv")
print("First 5 entries of the Iris dataset with selected columns:")
print(df_iris.filter(["Species", "SepalLengthCm", "SepalWidthCm"]).head())
.
Part 2: Analyzing "employees.csv"
# Step 2: Importing “employees.csv” dataset
# Using sort_values() function to arrange values in ascending and descending order
df_employees = pd.read_csv("employees.csv")
print("\nFirst 5 entries of the employees dataset:")
print(df_employees.head())

df_employees.sort_values(["Salary"], axis=0, ascending=[False], inplace=True)
print("\nEmployees dataset after sorting by 'Salary' in descending order:")
print(df_employees)

Part 3: Creating a New DataFrame and Using groupby()
# Creating a new DataFrame having 3 Columns “Name”, “Age”, “Address” and “Qualification”
# Using groupby() function to group data according to categories
data1 = {
    "Name": ["Jai", "Anuj", "Jai", "Princi", "Gaurav", "Anuj", "Princi", "Abhi"],
    "Age": [27, 24, 22, 32, 33, 36, 27, 32],
    "Address": ["Nagpur", "Kanpur", "Allahabad", "Kannuaj", "Jaunpur", "Kanpur", "Allahabad", "Aligarh"],
    "Qualification": ["Msc", "MA", "MCA", "Phd", "B.Tech", "B.com", "Msc", "MA"]
}
df_grouped = pd.DataFrame(data1)
print("\nOriginal DataFrame:")
print(df_grouped)

print("\nAfter Grouping by 'Name' and showing the first entry in each group:")
gk = df_grouped.groupby("Name")
print(gk.first())

-----------------------------------------------------------‐----
Aim: Finding Relationships Among Variables
-----------------------------------------------------------‐----
Part A: Correlation Analysis
Objective: To use the "data.csv" dataset to determine the strength and direction of linear relationships between pairs of variables.
Dataset Description: The "data.csv" dataset contains four columns: Duration, Pulse, Maxpulse, and (presumably a typo, should likely be another variable) Duration.

# Step 1: Using corr() function “Correlation” to show the relationship between multiple variables
import pandas as pd

# Load the dataset
d = pd.read_csv("data.csv")

# Calculate the correlation matrix
correlation_matrix = d.corr()

# Print the correlation matrix
print("Correlation matrix of the dataset:\n", correlation_matrix)

Part B: Regression Analysis
Objective: To apply linear regression to identify the relationship between an independent variable (from "data2.csv") and a dependent variable.
Dataset Description: The "data2.csv" dataset contains at least two columns, where the first column is treated as the independent variable (x) and the second as the dependent variable (y).

# Step 1: Importing all required libraries for Linear Regression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 2: Importing dataset file “data2.csv” using read_csv method
# Using iloc() function extracting rows based on integer-location indexing.
d = pd.read_csv("data2.csv")
x = d.iloc[:, 0].values.reshape(-1, 1)
y = d.iloc[:, 1].values.reshape(-1, 1)

# Step 3: Implementing Linear Regression
# Initialize the Linear Regression model
lr = LinearRegression()

# Train the model using the fit() function
lr.fit(x, y)

# Predict the dependent variable (y) based on the independent variable (x)
y_predicted = lr.predict(x)

# Visualize the data points and the regression line
plt.scatter(x, y, label="Original Data")
plt.plot(x, y_predicted, color="red", label="Linear Regression Line")
plt.xlabel("Independent Variable (Column 0)")
plt.ylabel("Dependent Variable (Column 1)")
plt.title("Linear Regression")
plt.legend()
plt.show()

-----------------------------------------------------------‐----
Aim: Probability and Probability Distributions
-----------------------------------------------------------‐----
Part A: Simulating a Probability Experiment (Rolling Dice)
Objective: To simulate the rolling of two dice using programming and visualize the probabilities of different outcomes.

# Step 1: Define a function to simulate rolling two dice n times
import numpy as np
import matplotlib.pyplot as plt

def roll_two_dice(n):
    """Simulates rolling two dice n times and returns a list of the sums."""
    rolls = []
    for _ in range(n):
        # Generate a random integer between 1 and 6 for each die and sum them
        two_dice_sum = np.random.randint(1, 7) + np.random.randint(1, 7)
        rolls.append(two_dice_sum)
    return rolls

# Step 2: Simulate a large number of dice rolls
num_rolls = 99000
dice_outcomes = roll_two_dice(num_rolls)

# Step 3: Create bins for the histogram
# We want bins centered around the possible sums (2 to 12)
bins = np.arange(1.5, 13.5, 1)

# Step 4: Plot the histogram of the sums
plt.hist(dice_outcomes, bins=bins, color="green", rwidth=0.8)
plt.grid(True, axis='y', linestyle='--')
plt.title("Sum of Two Dice Rolls")
plt.xlabel("Sum of Two Dice")
plt.ylabel("Frequency of Rolling Number")
plt.xticks(range(2, 13))  # Set x-axis ticks to the possible sums
plt.xlim(1.5, 12.5)      # Set x-axis limits to align with bins
plt.show()

Part B: Generating Random Numbers from Various Probability Distributions
Objective: To generate random numbers from normal, uniform, and exponential probability distributions and visualize their distributions.

a) Normal Distribution:
# a) Normal Distribution
import numpy as np
import matplotlib.pyplot as plt

mean = 100
sd = 15  # Corrected standard deviation value for better visualization
size = 100000
values = np.random.normal(mean, sd, size)
plt.hist(values, bins=100, density=True, alpha=0.6, color='blue') # Added density and alpha for better visualization
plt.axvline(values.mean(), color="k", linestyle="dashed", linewidth=2, label=f'Mean: {values.mean():.2f}')
plt.title("Normal Distribution")
plt.xlabel("Values")
plt.ylabel("Probability Density")
plt.legend()
plt.show()

b) Uniform Distribution:
# b) Uniform Distribution
import numpy as np
import matplotlib.pyplot as plt

low = -5
high = 5
size = 5000
uniform_values = np.random.uniform(low, high, size)
plt.hist(uniform_values, bins=50, density=True, alpha=0.6, color='orange') # Added alpha and color
plt.title("Uniform Distribution")
plt.xlabel("Values")
plt.ylabel("Probability Density")
plt.show()

c) Exponential Distribution:
# c) Exponential Distribution
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(10)
size = 10000
scale = 1.0  # The inverse of the rate (lambda)
exponential_sample = np.random.exponential(scale, size)
bins = np.arange(0, 10, 0.2) # Adjusted bin width for better visualization
plt.hist(exponential_sample, bins=bins, edgecolor="blue", alpha=0.7, density=True, color='lightblue') # Added density and alpha
plt.title("Exponential Distribution")
plt.xlabel("Values")
plt.ylabel("Probability Density")
plt.show()
-----------------------------------------------------------‐----
Aim: Decision Making under Uncertainty
-----------------------------------------------------------‐----
Part A: Develop a Decision Tree Model
Objective: To develop a decision tree model using the "Titanic.csv" dataset to predict a binary outcome (survival) based on various features. While this isn't a traditional decision tree for decision-making under uncertainty with explicit probabilities at each branch, it uses a decision tree classifier for prediction, which implicitly learns decision boundaries based on the data.
Dataset Description: The "Titanic.csv" dataset contains information about passengers on the Titanic, including features like age, gender, class, and survival status.

# Step 1: Importing libraries and reading the CSV file "Titanic.csv"
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv("Titanic.csv")
print("First 5 rows of the Titanic dataset:")
print(df.head())

# Step 2: Handling missing values
# Filling missing 'age' with the median
df['age'].fillna(df['age'].median(), inplace=True)
print("\nNull values in 'age' after filling:", df['age'].isnull().sum())

# Filling missing 'embarked' with the mode
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)
print("Null values in 'embarked' after filling:", df['embarked'].isnull().sum())

# Step 3: Converting categorical features into numerical using one-hot encoding
df = pd.get_dummies(df, columns=['sex', 'embarked', 'class', 'who'], drop_first=True)
print("\nDataFrame after one-hot encoding:")
print(df.head())

# Step 4: Splitting the dataset into training and testing subsets
X = df.drop('survived', axis=1)
y = df['survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("\nShape of training features:", X_train.shape)
print("Shape of testing features:", X_test.shape)
print("Shape of training target:", y_train.shape)
print("Shape of testing target:", y_test.shape)

# Step 5: Importing and training the Decision Tree Classifier
model = DecisionTreeClassifier(random_state=42) # Added random_state for reproducibility
model.fit(X_train, y_train)
print("\nDecisionTreeClassifier trained.")

# Step 6: Predicting on the test set and calculating accuracy
y_predicted = model.predict(X_test)
accuracy = accuracy_score(y_test, y_predicted)
print(f"\nAccuracy of the Decision Tree Classifier: {accuracy:.4f}")

Part B: Apply the Concept of Expected Value
Objective: To apply the concept of expected value to evaluate different decision alternatives and select the optimal one.
Source Code and Steps:
# Aim: b) Apply the concept of expected value to evaluate different decision alternatives and select the optimal one

def calculate_expected_value(probabilities, payoffs):
    """Calculates the expected value of a decision alternative."""
    expected_value = 0
    for prob, payoff in zip(probabilities, payoffs):
        expected_value += prob * payoff
    return expected_value

def select_optimal_decision(decision_alternatives, expected_values):
    """Selects the decision alternative with the highest expected value."""
    optimal_decision = decision_alternatives[0]
    max_expected_value = expected_values[0]
    for decision, expected_value in zip(decision_alternatives, expected_values):
        if expected_value > max_expected_value:
            optimal_decision = decision
            max_expected_value = expected_value
    return optimal_decision

# Define probabilities and payoffs for different decision alternatives
probabilities = [0.3, 0.5, 0.2]
payoffs = [[1000], [2000], [3000]]  # Payoffs for each alternative are in lists
decision_alternatives = ['A', 'B', 'C']

# Calculate the expected value for each decision alternative
expected_values = [calculate_expected_value(probabilities, payoff) for payoff in payoffs]

# Select the optimal decision
optimal_decision = select_optimal_decision(decision_alternatives, expected_values)

# Print the results
print("Probabilities:", probabilities)
print("Payoffs for each alternative:", payoffs)
print("Expected Values:", expected_values)
print("Optimal Decision:", optimal_decision)
-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
Aim: Sampling and Sampling Distributions
-----------------------------------------------------------‐----
-----------------------------------------------------------‐----
Part A: Conduct a Survey and Collect Data from a Sample Population
Objective: To demonstrate various sampling techniques using Python code. Note that actually conducting a real-life survey and collecting data is beyond the scope of this code snippet, but the code illustrates how different sampling methods can be implemented on a given population or dataset.

a) Simple Random Sampling:
# a) Random Sampling
import random as rd

# Define the population
population = list(range(1, 100))
sample_size = 10

# Perform simple random sampling
sample = rd.sample(population, sample_size)
print("Simple random sampling of 10 numbers are:", sample)

b) Systematic Sampling:
# b) Systematic Sampling
import numpy as np

# Define the population
population = np.arange(1, 100)
sample_size = 10

# Calculate the sampling interval
sampling_interval = len(population) // sample_size

# Choose a random starting point
start_point = np.random.randint(0, sampling_interval)

# Select the systematic sample
sample = population[start_point::sampling_interval]
print("Systematic or interval sampling of 10 numbers are:", sample)

c) Stratified Sampling:
# c) Stratified Sampling:
import pandas as pd
from sklearn.model_selection import train_test_split

# Step 1: Importing libraries and reading the CSV file "Titanic.csv"
df = pd.read_csv("Titanic.csv")

# Step 2: Splitting the dataset into training and testing sets using stratified sampling
stratify_by = 'sex'
train_df, test_df = train_test_split(df, test_size=0.3, stratify=df[stratify_by], random_state=42)

# Print the value counts of the stratification variable in both sets
print("Train dataset:\n", train_df[stratify_by].value_counts())
print("\nTest dataset:\n", test_df[stratify_by].value_counts())

d) Cluster Sampling:
# d) Cluster Sampling
import random as rd

# Define the population data
ppl_data = [10, 15, 20, 26, 29, 33, 35, 37, 41, 42, 46, 48, 50, 52, 58, 61, 64, 68, 72]
cluster_size = 5

# Choose a random starting point within the first cluster size
start_point = rd.randint(0, cluster_size - 1)

# Select clusters based on the starting point and cluster size
sample_data = []
for i in range(start_point, len(ppl_data), cluster_size):
    sample_data.append(ppl_data[i:i + cluster_size])

# Print the starting point and the selected clusters
print("Starting Point:", start_point)
print("Sampled Clusters:", sample_data)

# Aim: b) Use the Central Limit Theorem to analyze the sampling distribution of a sample mean and estimate population parameters.
import numpy as np
import matplotlib.pyplot as plt

# Define different sample sizes
num_samples = [1, 10, 50, 100]
sample_means = []

# Iterate through different sample sizes
for n in num_samples:
    np.random.seed(1)  # For reproducibility
    means_for_n = [np.mean(np.random.randint(-40, 40, n)) for _ in range(1000)]
    sample_means.append(means_for_n)

# Create a 2x2 subplot for the histograms
fig, axes = plt.subplots(2, 2, figsize=(8, 8))
axes = axes.flatten()  # Flatten the 2D array of axes for easy indexing

# Plot the distribution of sample means for each sample size
for i, means in enumerate(sample_means):
    axes[i].hist(means, bins=10, density=True, alpha=0.7, color='skyblue')
    axes[i].set_title(f'Sample Size = {num_samples[i]}')
    axes[i].set_xlabel('Sample Mean')
    axes[i].set_ylabel('Probability Density')

plt.tight_layout()
plt.show()

-----------------------------------------------------------‐----
Aim: Confidence Interval Estimation
-----------------------------------------------------------‐----
Part A: Calculate Confidence Intervals for Population Means or Proportions
Objective: To calculate a confidence interval for a population proportion using sample data and interpret the result.
Source Code and Steps:
# a) Calculate confidence intervals for population proportions
import statsmodels.api as sm
from statsmodels.stats.proportion import proportion_confint

# Given data:
# Number of successes (e.g., customers who made a purchase)
n_successes = 310
# Total number of observations (e.g., total customers surveyed)
n_observations = 1126
# Confidence level (1 - alpha) = 0.99, so alpha = 1 - 0.99 = 0.01

# Calculate the 99% confidence interval for the proportion
confidence_interval = proportion_confint(n_successes, n_observations, alpha=0.01, method='wilson')

# Print the confidence interval
print(f"99% Confidence Interval for the Population Proportion: {confidence_interval}")
.
Part B: Apply Bootstrapping Techniques to Estimate Confidence Intervals
Objective: To use bootstrapping to estimate the confidence interval for the mean of a population based on a sample.
Source Code and Steps:
# b) Apply bootstrapping techniques to estimate confidence intervals for non-parametric statistics
import numpy as np
import random

# Generate a sample from a population (in this case, a normal distribution)
np.random.seed(42)  # for reproducibility
population_sample = np.random.normal(loc=300.0, scale=20.0, size=1000)
print("Mean of the original sample:", np.mean(population_sample))

# Number of bootstrap resamples
n_bootstraps = 2000
# Size of each bootstrap resample (can be the same as the original sample size)
bootstrap_sample_size = len(population_sample)
bootstrap_means = []

# Perform bootstrapping
for _ in range(n_bootstraps):
    # Create a bootstrap resample by sampling with replacement from the original sample
    bootstrap_resample = random.choices(population_sample.tolist(), k=bootstrap_sample_size)
    # Calculate the mean of the bootstrap resample
    bootstrap_mean = np.mean(bootstrap_resample)
    bootstrap_means.append(bootstrap_mean)

# Calculate the confidence interval from the distribution of bootstrap means
confidence_level = 0.95
alpha = 1 - confidence_level
lower_percentile = alpha / 2 * 100
upper_percentile = (1 - alpha / 2) * 100

lower_bound = np.percentile(bootstrap_means, lower_percentile)
upper_bound = np.percentile(bootstrap_means, upper_percentile)

print(f"\nNumber of Bootstrap Resamples: {n_bootstraps}")
print(f"{confidence_level*100}% Bootstrap Confidence Interval for the Mean: ({lower_bound:.2f}, {upper_bound:.2f})")